---
title: "Class 4 script"
output: html_document
---

```{r setup, include=FALSE}
# Load libraries
library(tidyverse)
library(rpart)  # Install this:  install.packages("rpart")
library(rpart.plot) # Install this:  install.packages("rpart.plot")
```

## Agenda

1. Review the last project assignment. Remember that it is fine to turn these assignments in late (there is a small penalty). The final deadline is the start of the next module's class. 

2. Prepare for this module's modeling project.  

## Review Module 3 Project 

### Download data

```{r}
a <- read.csv("adviseinvest.csv")

glimpse(a)

```

Variables that represent *categories* (even if coded numerically) should be coded as factors. 

Variables that represent *counts* can be left as integers (or numeric data type). In some circumstances you might choose to represent a count as a factor, but in general such variables should be numeric.

### Q3

Cleaning/preparing the data:

- Remove rows with NAs.
- Remove the row with the single outlier in num_accts.
- Remove the rows with negative income values.
- Make 4 non-binary categorical variables into factors.

```{r}  
a_clean <- a |>                         # Notice I am saving the clean data in a new object!
  mutate(job = factor(job),
         product = factor(product),      # Allow R to assign default factor levels
         sav_acct = factor(sav_acct),
         chk_acct = factor(chk_acct)) |> 
  filter(income > 0,                     # Filter out rows with mistaken values
         num_accts < 5) |> 
  na.omit()                                # filter out rows with missing values

```

Calculate and report the mean of "answered" after cleaning the dataset.  Because "answered" is coded 0/1 this is as simple as calculating the proportion of 1s in that vector with mean().

```{r}

mean(a_clean$answered)  

```


## Q4

Recode the variable "answered" which is coded 0/1.  Turn 1 into  "yes" and  0 into "no," then turn it into a factor. (This is worth doing because not all audiences will know what a 0 vs. a 1 means for "answered.")  Hint:  use ifelse(), which is very similar to the Excel function if().  The syntax is:  ifelse(test, x, y). In English:  if test is TRUE, then return x, otherwise return y.

```{r}
a_clean2 <- a_clean |>               
         mutate(answered = factor(answered))
summary(a_clean2)
```


Plot "answered" against "num_accts." Take care to choose the appropriate plot type for showing the relationship between a categorical and a numeric or count variable.  Which plot type would be appropriate: histogram? scatterplot? boxplot? line plot? barplot?

Boxplot! A boxplot is the default choice for plotting a categorical variable against a continuous variable. Here it makes sense to treat checking account as a continuous variable since it is a *count* of the number of customer checking accounts. 

```{r}
a_clean2 |>                                  # Notice I am not saving the plot
  ggplot(aes(x = answered, y = num_accts)) + 
  geom_boxplot() +
  labs(title = "answered ~ num_accts")
```

Interpretation: people who answered tend to have more accounts than people who did not answer. 

We could do a little bit more analysis:

```{r}
a_clean2 |> 
  group_by(answered) |> 
  summarize(avg_accts = mean(num_accts),
            median_accts = median(num_accts)) 
```

### Q5
  
Plot "answered" against "mobile." 

Counts: 

```{r}
a_clean2 |>                               # I'm not saving the plot here
  mutate(mobile = factor(mobile)) |>
  count(mobile, answered) |>
  ggplot(aes(mobile, n, fill = answered)) + 
  geom_col(position = "dodge") +
  labs(title = "Count of answered by mobile")


a=5
b=factor(a)
print(a)


```

Proportions:

```{r}
a_clean2 |>
  mutate(mobile = factor(mobile)) |>
  count(mobile, answered) |>
  group_by(mobile) |> 
  mutate(proportion = n/sum(n)) |> 
  ggplot(aes(mobile, proportion, fill = answered)) + 
  geom_col(position = "dodge") +
  labs(title = "Proportion of answered by mobile")

```

The proportion plot looks very different! But the information is really the same. The scaling is different because the height of the bars in the proportion plot is relative to the category. This makes it easier to interpret. People with mobile phones answer the calls more often than people without mobile phones. Answerers are in the majority in each case, but much more so for mobile phone users.

## Module 4 Project 

What do you need to be able to do?

    a. Understand model accuracy.  Why is it a performance metric for classification and not regression?
    
    b. Calculate accuracy for a simple majority class model (this is the same as calculating the proportion of the majority class in a binary variable). Consider: x <- c(1, 1, 1, 0, 0).  What is the majority class? What is the proportion of the majority class in x?
    
    c. Fit a tree model of the target with just one predictor variable and calculate the accuracy of this model.
    
    d. Interpret a tree model, and calculate information gain.
    
    e. Fit a tree model of the target using all the predictors, then:  create a visualization of the tree and identify the top 3 most important predictors in this model.
    
    f. Answer the question: how do these models compare to majority class prediction?
    
    g. Answer: How will you use a classification model as part of a solution to the AdviseInvest case?

We will use the MegaTelCo data for this demonstration.

```{r}
# Load and clean data

m <- read_csv(file = "megatelco.csv")

m_clean <- m |> 
  mutate(reported_satisfaction = factor(reported_satisfaction),
         reported_usage_level = factor(reported_usage_level),
         considering_change_of_plan = factor(considering_change_of_plan),
         leave = factor(leave), 
         college = ifelse(college=="one", "yes", "no"),
         college = factor(college)) |> 
  filter(income > 0,
         house > 0,
         handset_price < 1500) |> 
  select(-id) |> # Remove ID because it is not relevant as a predictor
  na.omit ()

# Check whether the operation was successful
summary(m_clean)

```

### Calculate distribution of the outcome

What is the proportion of people who churned?

```{r}
# Could use the numbers from the above summary:

7470 / (7470 + 7521)
 
# Other methods
ifelse(m_clean$leave == "LEAVE", 1, 0) |> 
  mean()

mean(m_clean$leave=="LEAVE")

```

Obviously, if `leave` were expressed as 0/1, then:  `mean(m_clean$leave).`

Why should we care about this proportion?

1. An important step in EDA is to understand the distribution of the target variable.

2. The majority class in the target variable will serve as an important benchmark for model performance. If we used what we'll call a "majority class classifier"---this consists in always predicting the majority class, which in this case is `STAY`---we would be correct 1 - .49 or 51% of the time.  Another way of saying this is that majority class classifier in the MegaTelCo case would result in accuracy of .51.  

Accuracy is defined as the proportion of correctly predicted labels. It is a commonly used error metric for evaluating classifier performance.

Think about why a majority class model in this case would have an accuracy of .51.

Whatever later model we develop should have better accuracy than this performance benchmark.

### Fit a tree model

Use just one variable, `income`. We'll call this the "money tree." What is the accuracy of the money tree?  

```{r}
# Needs rpart library! Make sure you have run:
# library(rpart)

(money_tree <- rpart(formula = leave ~ income, 
                     data = m_clean))

# The formula, y ~ x, means "y explained by x." 
# This is a common syntax for defining a model in R. 

```

Plot the money tree.

```{r}
# Needs rpart.plot library! Make sure you have run:
# library(rpart.plot)

rpart.plot(x = money_tree)

# x is a model object created by rpart().

```

What is the accuracy of the money_tree? Use these steps to calculate accuracy.

1.  Get predictions. Use `predict()` with the `type` argument set to "class." The syntax is: `predict(model, type = "class")`. This will return predicted class labels. 

```{r}
predict(money_tree, 
        type = "class") |> 
  head ()

# The point of this code chunk is to show how predict() works.
# We are using head() to avoid printing all observations to screen

```

2. Create a vector comparing model predictions to the observed outcomes for each row. If the prediction is the same as the observed, then the result will be TRUE; if it is not the same, the result will be FALSE.  This vector will show whether the model correctly predicted the outcome for each row.

```{r}
(predict(money_tree, type = "class")==m_clean$leave) |> 
  head ()

# The point of this is to show how the code works in this step.
# We are using head() to avoid printing all observations to screen

```

3. Take the mean of that vector to calculate the proportion of Ts.  This is the model's accuracy.

```{r}
(predict(money_tree, type = "class")== m_clean$leave) |> 
  mean () #.57

# mean() will calculate the proportion of TRUEs in this vector. 
```

The money tree model is more accurate than majority class prediction! .51 has increased to .57.

### Calculate information gain (IG)

IG is a metric telling you how much more homogeneous the groups in a dataset are after splitting a parent into two children. IG relies on the concept of entropy.  These are the quantities we need to compute:

- entropy(parent): entropy in the parent, prior to the split.
- entropy(c1): entropy in the first child.
- p(c1): the proportion of observations from the parent that wind up in the first child after the split.
- entropy(c2): entropy in the second child.
- p(c2): the proportion of observations from the parent that wind up in the second child after the split.

The formula is:

IG = entropy(parent) - [p(c1) * entropy(c1) + p(c2) * entropy(c2)]

Recall that entropy for any group is defined as:

- p1 * log2(p1) - p2 * log2(p2)

What is the IG associated with  the first split in this tree? 

For this exercise, use log base 2: `log2(x)`.  

```{r}  
money_tree

# The first node with an asterisk, indicating a terminal node, is node 2, defined as:  2) income>=99993.  

(parent_entropy <- -.49 * log2(.49) - .51 * log2(.51))

(child1_entropy <- -.59 * log2(.59) - .41 * log2(.41))

(child2_entropy <- -.44 * log2(.44) - .56 * log2(.56))

(p_c1 <- 1691/4994)

(p_c2 <- 3303/4994)

# IG = entropy(parent) - [p(c1) * entropy(c1) + p(c2) * entropy(c2)]

parent_entropy - (p_c1 * child1_entropy + p_c2 * child2_entropy)

```

### Visualize tree model.

Fit a tree model of the outcome using all the predictors and visualize the model using `rpart.plot()`. Two arguments to `rpart.plot()` will be useful for creating a legible plot:  `tweak` and `roundint`.  Use the setting recommended in the quiz.  

Based on the plot, what are the most important predictors in this model?

Note: shorthand to add all predictors is: "."

You will get the wrong (different) answer if you have not modeled categorical variables correctly as factors!

Notice that in the module 4 project template there is a code chunk provided for cleaning and preparing the data. Use it! 

```{r}
(leafy_tree <- rpart(formula = leave ~., 
                     data = m_clean,
                     maxdepth = 5,
                     minbucket = 10))

rpart.plot(x = leafy_tree, 
           tweak = 1, 
           roundint = T) 

# You can play around with different values of tweak for legibility
```

The splits higher in the tree are those that maximize IG at any given step. Therefore, the variables closest to the root node are the most important for predicting the target variable: house, income and overage.

### Compare models using accuracy

What is the accuracy of the leafy_tree model? Is it better than the money_tree or better than majority class prediction? 

```{r}
(predict(money_tree, type = "class")== m_clean$leave) |> 
  mean () #.57

(predict(leafy_tree, type = "class")== m_clean$leave) |> 
  mean () #.71
```

Yes, both models are better than the majority class classifier.  The leafy tree model is the best.
